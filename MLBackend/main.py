import shutil
import os
import uuid
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from pydantic import BaseModel
from typing import Optional
import requests


from core.model_manager import model_manager
from services.whisper_service import WhisperService
from services.llava_service import LlavaService
from services.clip_service import ClipService
import uvicorn

class TranscribeRequest(BaseModel):
    video_url: str
    language: Optional[str] = "ru"
    sync: bool = True
    max_segment_duration_seconds: int = 15


app = FastAPI(title="Pure ML Backend")

TEMP_DIR = "temp_uploads"
os.makedirs(TEMP_DIR, exist_ok=True)


@app.post("/transcribe")
async def transcribe_video(request: TranscribeRequest):
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç JSON —Å URL, —Å–∫–∞—á–∏–≤–∞–µ—Ç –≤–∏–¥–µ–æ, –ø—Ä–æ–≥–æ–Ω—è–µ—Ç —á–µ—Ä–µ–∑ Whisper.
    """
    filename = f"{uuid.uuid4()}_video.mp4"
    temp_path = os.path.join(TEMP_DIR, filename)
    
    print(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–∏–¥–µ–æ: {request.video_url}")
    
    try:
        with requests.get(request.video_url, stream=True) as r:
            r.raise_for_status()
            with open(temp_path, 'wb') as f:
                for chunk in r.iter_content(chunk_size=8192): 
                    f.write(chunk)
        
        print("‚úÖ –í–∏–¥–µ–æ —Å–∫–∞—á–∞–Ω–æ, –∑–∞–ø—É—Å–∫ Whisper...")

        whisper = model_manager.get_model("whisper", WhisperService)

        segments = whisper.transcribe(temp_path, language=request.language)
        
        full_text = " ".join([s.text for s in segments])
        segments_data = [{"start": s.start, "end": s.end, "text": s.text} for s in segments]
        
        return {"transcript": full_text, "segments": segments_data}
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if os.path.exists(temp_path):
            os.remove(temp_path)


@app.post("/clip_embed")
async def get_clip_features(file: UploadFile = File(...)):
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫—É, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä (embeddings).
    CLIP –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è, Whisper/LLaVA –≤—ã–≥—Ä—É–∂–∞—é—Ç—Å—è.
    """
    temp_path = os.path.join(TEMP_DIR, f"{uuid.uuid4()}_{file.filename}")
    try:
        with open(temp_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
            
        clip = model_manager.get_model("clip", ClipService)
        features = clip.get_image_features(temp_path)
        
        return {"features": features}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if os.path.exists(temp_path):
            os.remove(temp_path)


@app.post("/llava_describe")
async def describe_image(
    file: UploadFile = File(...), 
    prompt: str = Form("–î–∞–π –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —ç—Ç–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–∏. –û—Ç–¥–µ–ª—å–Ω–æ —Å–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Å—è –Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞—Ö" \
    " –∏ –∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–∏ –¥—Ä—É–≥ —Å –¥—Ä—É–≥–æ–º.")
):
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ—Ç –∫–∞—Ä—Ç–∏–Ω–∫—É –∏ –ø—Ä–æ–º–ø—Ç.
    LLaVA –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≤—ã–≥—Ä—É–∂–∞—é—Ç—Å—è.
    """
    temp_path = os.path.join(TEMP_DIR, f"{uuid.uuid4()}_{file.filename}")
    try:
        with open(temp_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
            
        llava = model_manager.get_model("llava", LlavaService)
        response = llava.analyze(temp_path, prompt)
        clean_text = response.split("[/INST]")[-1].strip()
        
        return {"description": clean_text}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        if os.path.exists(temp_path):
            os.remove(temp_path)

if __name__ == "__main__":
    print("üöÄ –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–∞ ML Backend...")
    uvicorn.run(app, host="127.0.0.1", port=8000, workers=1)