# hybrid_rag_server.py
import os
import requests
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional, Tuple
from qdrant_client import QdrantClient
from qdrant_client.models import Filter, FieldCondition, MatchValue
import google.generativeai as genai
import uvicorn
from rank_bm25 import BM25Okapi
from dotenv import load_dotenv
import numpy as np
from collections import defaultdict
load_dotenv()
from fastapi.middleware.cors import CORSMiddleware
import traceback

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
ML_BACKEND_URL = "http://localhost:8000"
QDRANT_URL = "http://localhost:6333"
COLLECTION_NAME = "knowledge_base"
MODEL_NAME = "paraphrase-multilingual-mpnet-base-v2"

# Allow overriding Gemini model via env; default to a performant model
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-2.5-flash-lite")
api_key = os.getenv("GEMINI_API_KEY")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–∫—Å–∏ (—Å–æ–≤–º–µ—Å—Ç–∏–º–∞—è –ª–æ–≥–∏–∫–∞)
os.environ['NO_PROXY'] = 'localhost,127.0.0.1,0.0.0.0,::1'
os.environ['no_proxy'] = 'localhost,127.0.0.1,0.0.0.0,::1'

# –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –ë–ï–ó –ø—Ä–æ–∫—Å–∏ –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
local_session = requests.Session()
local_session.trust_env = False

app = FastAPI(title="Hybrid RAG Knowledge Base API")

# CORS (—Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ –Ω–∞ –¥—Ä—É–≥–æ–º —Ö–æ—Å—Ç–µ)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# BM25 –∫—ç—à
bm25_cache = {
    "index": None,
    "documents": [],
    "metadata": []
}

# --- –ú–û–î–ï–õ–ò –î–ê–ù–ù–´–• ---
class QueryRequest(BaseModel):
    query: str
    top_k: int = 5
    category: Optional[str] = None
    use_hybrid: bool = True
    # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è —Å —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–æ–º: —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ –ø—Ä–∏—Å—ã–ª–∞–µ—Ç `use_entities`
    use_entities: bool = True
    # –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ (–µ—Å–ª–∏ –∫—Ç–æ-—Ç–æ –≤—Å—ë –µ—â—ë —à–ª—ë—Ç —Å—Ç–∞—Ä–æ–µ –∏–º—è):
    use_entity_extraction: Optional[bool] = None
    # –ù–æ–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä: –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0.0 - 1.0)
    similarity: float = 0.3

class RetrievedDocument(BaseModel):
    text: str
    score: float
    category: str
    lesson: str
    chunk_id: int
    search_method: str

class RAGResponse(BaseModel):
    answer: str
    sources: List[RetrievedDocument]
    query: str
    extracted_entities: Optional[List[str]] = None

# --- –£—Ç–∏–ª–∏—Ç—ã / core functions ---

def configure_client():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–∫—Å–∏ –∏ Gemini API"""
    proxy_url = os.getenv("GEMINI_PROXY", "http://127.0.0.1:12334")
    if proxy_url:
        no_proxy = "localhost,127.0.0.1,0.0.0.0,::1,localhost:8000,localhost:6333,127.0.0.1:8000,127.0.0.1:6333"
        os.environ['http_proxy'] = proxy_url
        os.environ['https_proxy'] = proxy_url
        os.environ['HTTP_PROXY'] = proxy_url
        os.environ['HTTPS_PROXY'] = proxy_url
        os.environ['no_proxy'] = no_proxy
        os.environ['NO_PROXY'] = no_proxy
        print(f"[Gemini] –ù–∞—Å—Ç—Ä–æ–µ–Ω –ø—Ä–æ–∫—Å–∏: {proxy_url}")
    if api_key:
        genai.configure(api_key=api_key, transport="rest")
        print("[Gemini] API –∫–ª—é—á –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

def get_embedding(text: str) -> list:
    """–ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ ML backend"""
    try:
        response = local_session.post(
            f"{ML_BACKEND_URL}/text_embed",
            json={"text": text, "model_name": MODEL_NAME},
            timeout=30
        )
        response.raise_for_status()
        return response.json()["embedding"]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {str(e)}")

def load_bm25_index() -> Tuple[Optional[BM25Okapi], List[str], List[dict]]:
    """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ Qdrant –∏ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å BM25 –∏–Ω–¥–µ–∫—Å (idempotent)"""
    if bm25_cache["index"] is not None and bm25_cache["documents"]:
        return bm25_cache["index"], bm25_cache["documents"], bm25_cache["metadata"]
    try:
        client = QdrantClient(
            host="localhost",
            port=6333,
            timeout=30,
            prefer_grpc=False
        )
        collection_info = client.get_collection(collection_name=COLLECTION_NAME)
        total_points = collection_info.points_count
        print(f"[BM25] –ó–∞–≥—Ä—É–∂–∞–µ–º {total_points} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è BM25 –∏–Ω–¥–µ–∫—Å–∞...")
        documents = []
        metadata = []
        offset = None
        while True:
            results = client.scroll(
                collection_name=COLLECTION_NAME,
                limit=100,
                offset=offset,
                with_payload=True
            )
            points, next_offset = results
            if not points:
                break
            for point in points:
                # –∑–∞—â–∏—Ç–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø–æ–ª–µ–π
                text = point.payload.get("text", "")
                cat = point.payload.get("category", "unknown")
                lesson = point.payload.get("lesson", "unknown")
                chunk_id = point.payload.get("chunk_id", -1)
                documents.append(text)
                metadata.append({
                    "id": point.id,
                    "category": cat,
                    "lesson": lesson,
                    "chunk_id": chunk_id
                })
            if next_offset is None:
                break
            offset = next_offset
        tokenized_docs = [doc.lower().split() for doc in documents]
        bm25_index = BM25Okapi(tokenized_docs) if documents else None
        bm25_cache["index"] = bm25_index
        bm25_cache["documents"] = documents
        bm25_cache["metadata"] = metadata
        print(f"[BM25] –ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω –Ω–∞ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö")
        return bm25_index, documents, metadata
    except Exception as e:
        print(f"[BM25] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        return None, [], []

def extract_entities_and_keywords(query: str) -> List[str]:
    """–ò–∑–≤–ª–µ—á—å –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ Gemini (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)"""
    if not api_key:
        # –ü—Ä–æ—Å—Ç–∞—è —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏—è –±–µ–∑ API
        return [word for word in query.lower().split() if len(word) > 3]
    try:
        model = genai.GenerativeModel(model_name=GEMINI_MODEL)
        prompt = f"""–ò–∑–≤–ª–µ–∫–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –∫–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã, –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.
–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û —Å–ø–∏—Å–æ–∫ —Ç–µ—Ä–º–∏–Ω–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π.

–ó–∞–ø—Ä–æ—Å: {query}

–¢–µ—Ä–º–∏–Ω—ã:"""
        response = model.generate_content(
            contents=prompt,
            generation_config=genai.GenerationConfig(temperature=0.1, max_output_tokens=200)
        )
        terms = [t.strip() for t in response.text.strip().split(',') if t.strip()]
        terms.extend([word for word in query.lower().split() if len(word) > 2])
        terms = list(dict.fromkeys(terms))  # —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ø–æ—Ä—è–¥–æ–∫, —É–±—Ä–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã
        print(f"[Entity] –ò–∑–≤–ª–µ—á–µ–Ω–æ —Ç–µ—Ä–º–∏–Ω–æ–≤: {terms}")
        return terms
    except Exception as e:
        print(f"[Entity] –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {e}")
        return [word for word in query.lower().split() if len(word) > 2]

def bm25_search(query: str, top_k: int = 10, similarity: float = 0.0) -> List[dict]:
    """BM25 —Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫. –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é –∏ —Ñ–∏–ª—å—Ç—Ä –ø–æ similarity (0..1)"""
    bm25_index, documents, metadata = load_bm25_index()
    if bm25_index is None:
        return []
    query_tokens = query.lower().split()
    scores = bm25_index.get_scores(query_tokens)
    if len(scores) == 0:
        return []
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø–æ –º–∞–∫—Å–∏–º—É–º—É
    max_score = float(np.max(scores)) if np.max(scores) > 0 else 1.0
    normalized_scores = scores / max_score if max_score != 0 else scores
    # –í—ã–±–∏—Ä–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –ø–æ top_k –∏ —Ñ–∏–ª—å—Ç—Ä—É –ø–æ similarity
    sorted_idx = np.argsort(normalized_scores)[::-1]
    results = []
    count = 0
    for idx in sorted_idx:
        if count >= top_k:
            break
        ns = float(normalized_scores[idx])
        if ns < similarity:
            continue
        if scores[idx] > 0:
            results.append({
                "text": documents[idx],
                "score": float(scores[idx]),
                "normalized_score": ns,
                "category": metadata[idx]["category"],
                "lesson": metadata[idx]["lesson"],
                "chunk_id": metadata[idx]["chunk_id"],
                "search_method": "bm25"
            })
            count += 1
    return results

def vector_search(query: str, top_k: int = 10, category: Optional[str] = None, similarity: float = 0.3) -> List[dict]:
    """–í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –≤ Qdrant —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –ø–æ—Ä–æ–≥–∞ score_threshold=similarity"""
    try:
        client = QdrantClient(host="localhost", port=6333, timeout=30, prefer_grpc=False)
        query_vector = get_embedding(query)
        query_filter = None
        if category:
            query_filter = Filter(must=[FieldCondition(key="category", match=MatchValue(value=category))])
        results = client.query_points(
            collection_name=COLLECTION_NAME,
            query=query_vector,
            limit=top_k,
            query_filter=query_filter,
            with_payload=True,
            score_threshold=similarity  # –Ω–∞–ø—Ä—è–º—É—é –∏—Å–ø–æ–ª—å–∑—É–µ–º similarity –∫–∞–∫ –ø–æ—Ä–æ–≥ –¥–ª—è —Å–∫–æ—Äa
        )
        documents = []
        for point in results.points:
            documents.append({
                "text": point.payload.get("text", ""),
                "score": point.score,
                "category": point.payload.get("category", "unknown"),
                "lesson": point.payload.get("lesson", "unknown"),
                "chunk_id": point.payload.get("chunk_id", -1),
                "search_method": "vector"
            })
        return documents
    except Exception as e:
        print(f"[Vector] –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
        return []

def hybrid_search(
    query: str,
    top_k: int = 5,
    category: Optional[str] = None,
    use_entities: bool = True,
    similarity: float = 0.3
) -> Tuple[List[dict], List[str]]:
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: –≤–µ–∫—Ç–æ—Ä–Ω—ã–π + BM25 —Å –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏–µ–π —Å—É—â–Ω–æ—Å—Ç–µ–π
    –ü—Ä–∏–º–µ–Ω—è–µ—Ç similarity –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–∞–∂–¥–æ–≥–æ –º–µ—Ç–æ–¥–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (results_list, extracted_terms)
    """
    extracted_terms = extract_entities_and_keywords(query) if use_entities else []
    vector_results = vector_search(query, top_k=top_k * 2, category=category, similarity=similarity)
    bm25_query = query
    if extracted_terms:
        bm25_query = query + " " + " ".join(extracted_terms[:5])
    bm25_results = bm25_search(bm25_query, top_k=top_k * 2, similarity=similarity)

    combined = {}
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è vector_results –ø–æ max –≤ —Ç–µ–∫—É—â–µ–º —Å–µ—Ç–µ
    if vector_results:
        max_vector_score = max((r["score"] for r in vector_results), default=0)
        for r in vector_results:
            key = r["text"][:200]
            normalized = (r["score"] / max_vector_score) if max_vector_score > 0 else 0
            combined.setdefault(key, {**r, "vector_score": normalized * 0.6, "bm25_score": 0, "search_method": r.get("search_method", "vector")})

    if bm25_results:
        max_bm25_score = max((r.get("normalized_score", 0) for r in bm25_results), default=0)
        # note: bm25_results already filtered by similarity and contain normalized_score
        for r in bm25_results:
            key = r["text"][:200]
            normalized = r.get("normalized_score", 0)
            if key in combined:
                combined[key]["bm25_score"] = normalized * 0.4
                combined[key]["search_method"] = "hybrid"
            else:
                combined.setdefault(key, {**r, "vector_score": 0, "bm25_score": normalized * 0.4, "search_method": "bm25"})

    final_results = []
    for v in combined.values():
        v["score"] = v.get("vector_score", 0) + v.get("bm25_score", 0)
        if v.get("search_method") == "hybrid":
            v["score"] *= 1.2
        final_results.append(v)
    final_results.sort(key=lambda x: x["score"], reverse=True)
    return final_results[:top_k], extracted_terms

def retrieve_documents(
    query: str,
    top_k: int = 5,
    category: Optional[str] = None,
    use_hybrid: bool = True,
    use_entities: bool = True,
    similarity: float = 0.3
) -> Tuple[List[dict], List[str]]:
    """Wrapper to choose retrieval method (passes similarity threshold)"""
    if use_hybrid:
        return hybrid_search(query, top_k=top_k, category=category, use_entities=use_entities, similarity=similarity)
    # vector only (apply similarity)
    return vector_search(query, top_k=top_k, category=category, similarity=similarity), []

def _ensure_citation_markers_in_answer(answer: str, num_sources: int) -> str:
    """
    –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞: –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –≤—Å—Ç–∞–≤–∏–ª–∞ —Å—Å—ã–ª–æ–∫, –¥–æ–±–∞–≤–ª—è–µ–º [1], [2] –≤ –∫–æ–Ω—Ü–µ –∞–±–∑–∞—Ü–µ–≤.
    –ù–æ —Å—Ç–∞—Ä–∞–µ–º—Å—è –Ω–µ –ª–æ–º–∞—Ç—å —É–∂–µ –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏.
    """
    # –µ—Å–ª–∏ —É–∂–µ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –º–µ—Ç–∫–∞ [1], —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –≤—Å—Ç–∞–≤–∏–ª–∞ —Å—Å—ã–ª–∫–∏
    if "[" in answer and any(f"[{i}]" in answer for i in range(1, num_sources+1)):
        return answer
    # –î–æ–±–∞–≤–∏–º –≤ –∫–æ–Ω–µ—Ü –∞–±–∑–∞—Ü–µ–≤ –º–µ—Ç–∫–∏ –ø–æ –æ—á–µ—Ä–µ–¥–∏
    parts = [p.strip() for p in answer.split("\n\n") if p.strip()]
    if not parts:
        return answer
    out_parts = []
    idx = 1
    for p in parts:
        marker = f" [{idx}]" if idx <= num_sources else ""
        out_parts.append(p + marker)
        if idx < num_sources:
            idx += 1
    return "\n\n".join(out_parts)

def generate_answer(query: str, context_docs: List[dict], extracted_entities: List[str] = None) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ Gemini —Å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π –≤—Å—Ç–∞–≤–ª—è—Ç—å —Å—Å—ã–ª–∫–∏ –≤–∏–¥–∞ [1], [2] –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—Å—Ç–∞.
    –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –ø–µ—Ä–µ–¥–∞—é—Ç—Å—è –≤ –ø–æ—Ä—è–¥–∫–µ context_docs (1..N).
    """
    num_sources = len(context_docs)
    if not api_key:
        # –í–µ—Ä–Ω—ë–º –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç, –Ω–æ —Å —Ü–∏—Ç–∞—Ç–∞–º–∏-–º–µ—Ç–∫–∞–º–∏, —á—Ç–æ–±—ã —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥ –º–æ–≥ —Å—Å—ã–ª–∞—Ç—å—Å—è
        context_preview = "\n\n".join([
            f"[{i+1}] [{doc['category']} / {doc['lesson']}] (score: {doc.get('score', 0):.2f})\n{doc['text'][:500]}"
            for i, doc in enumerate(context_docs)
        ])
        ans = f"‚ö†Ô∏è Gemini API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –ù–∞–π–¥–µ–Ω–æ {len(context_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:\n\n{context_preview}"
        # ensure markers
        return _ensure_citation_markers_in_answer(ans, num_sources)

    context = "\n\n---\n\n".join([
        f"–ò—Å—Ç–æ—á–Ω–∏–∫ {i+1} [{doc['category']} / {doc['lesson']}]:\n{doc['text']}"
        for i, doc in enumerate(context_docs)
    ])
    entities_info = ""
    if extracted_entities:
        entities_info = f"\n\n–ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–∑ –∑–∞–ø—Ä–æ—Å–∞: {', '.join(extracted_entities[:10])}"

    # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ü–∏—Ñ—Ä–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ [1], [2] —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–æ—Ä—è–¥–∫—É –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    prompt = f"""–¢—ã ‚Äî AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é –∏ ML.
–í–æ–ø—Ä–æ—Å: {query}{entities_info}

–ö–æ–Ω—Ç–µ–∫—Å—Ç (–∏—Å—Ç–æ—á–Ω–∏–∫–∏ –ø—Ä–æ–Ω—É–º–µ—Ä–æ–≤–∞–Ω—ã –≤ –ø–æ—Ä—è–¥–∫–µ –≤–∞–∂–Ω–æ—Å—Ç–∏):
{context}

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞:
- –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
- –í —Ç–µ–∫—Å—Ç–µ **–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ** –ø—Ä–æ—Å—Ç–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [1], [2] –∏ —Ç.–¥., –≥–¥–µ —Ü–∏—Ñ—Ä–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø–æ—Ä—è–¥–∫–æ–≤–æ–º—É –Ω–æ–º–µ—Ä—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–∑ –ø–µ—Ä–µ–¥–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
- –°—Å—ã–ª–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤—Å—Ç–∞–≤–ª–µ–Ω—ã —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π/—Ñ—Ä–∞–∑, –≥–¥–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä: '... —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–∞–∫ [1].'
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∫–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç—Å—è –∏–∑ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –º–æ–∂–Ω–æ –ø–æ—Å—Ç–∞–≤–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Å—ã–ª–æ–∫: [1][3].
- –ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –¥–∞—ë—Ç –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞, —á–µ—Å—Ç–Ω–æ —Å–∫–∞–∂–∏, —á—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏ —É–∫–∞–∂–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –æ–ø–∏—Ä–∞–ª—Å—è.
- –ü–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –±—É–¥—å –ø–æ–Ω—è—Ç–Ω—ã–º –∏ –∫—Ä–∞—Ç–∫–∏–º, –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º.
- –ù–µ –¥–æ–±–∞–≤–ª—è–π –≤–Ω–µ—à–Ω–∏—Ö URL-–∞–¥—Ä–µ—Å–æ–≤ –≤ –æ—Ç–≤–µ—Ç ‚Äî —Ç–æ–ª—å–∫–æ —Ü–∏—Ñ—Ä–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏ [n].

–û—Ç–≤–µ—Ç:"""

    try:
        model = genai.GenerativeModel(model_name=GEMINI_MODEL)
        response = model.generate_content(
            contents=prompt,
            generation_config=genai.GenerationConfig(
                temperature=0.4,
                top_p=0.8,
                max_output_tokens=4096,
            )
        )
        text = response.text.strip()
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –≤—Å—Ç–∞–≤–∏–ª–∞ –º–µ—Ç–∫–∏ ‚Äî –¥–æ–±–∞–≤–∏–º –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–æ–π
        text = _ensure_citation_markers_in_answer(text, num_sources)
        return text
    except Exception as e:
        print(f"[Gemini] –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}\n{traceback.format_exc()}")
        # fallback: –ø–æ–∫–∞–∑–∞—Ç—å –Ω–µ–±–æ–ª—å—à–æ–π –ø—Ä–µ–≤—å—é –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç–∫–∏
        context_preview = "\n\n".join([
            f"[{i+1}] {doc['category']} / {doc['lesson']} (score: {doc.get('score',0):.2f})\n{doc['text'][:300]}"
            for i, doc in enumerate(context_docs)
        ])
        err_text = f"‚ö†Ô∏è –û—à–∏–±–∫–∞ Gemini API: {str(e)}\n\n–ù–∞–π–¥–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏:\n\n{context_preview}"
        return _ensure_citation_markers_in_answer(err_text, num_sources)

# --- API endpoints ---

@app.get("/categories")
async def get_categories():
    if not bm25_cache["metadata"]:
        load_bm25_index()
    categories = sorted(list({m["category"] for m in bm25_cache["metadata"] if m.get("category")}))
    return {"categories": categories}

@app.get("/health")
async def health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –≤–Ω–µ—à–Ω–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤"""
    status = {
        "ml_backend": False,
        "qdrant": False,
        "gemini_api": bool(api_key),
        "bm25_index": bm25_cache["index"] is not None
    }
    try:
        response = local_session.get(f"{ML_BACKEND_URL}/docs", timeout=2)
        status["ml_backend"] = response.status_code == 200
    except:
        pass
    try:
        response = local_session.get(f"{QDRANT_URL}/collections", timeout=5)
        if response.status_code == 200:
            collections = response.json()
            names = [c["name"] for c in collections.get("result", {}).get("collections", [])]
            status["qdrant"] = COLLECTION_NAME in names
    except:
        pass
    return status

@app.post("/rebuild_bm25")
async def rebuild_bm25():
    """–ü–µ—Ä–µ—Å—Ç—Ä–æ–∏—Ç—å BM25 –∏–Ω–¥–µ–∫—Å (endpoint)"""
    bm25_cache["index"] = None
    bm25_cache["documents"] = []
    bm25_cache["metadata"] = []
    load_bm25_index()
    return {"status": "ok", "documents_count": len(bm25_cache["documents"])}

@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ: –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏ –ø–µ—Ä–µ—Å—Ç—Ä–æ–π–∫–∞ –∏–Ω–¥–µ–∫—Å–∞"""
    configure_client()

    # –°—Ç–∞—Ä—Ç–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
    print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ—Ä–≤–∏—Å–æ–≤ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ:")
    try:
        health_status = await health()
        for service, st in health_status.items():
            emoji = "‚úÖ" if st else "‚ùå"
            print(f"  {emoji} {service}: {st}")
    except Exception as e:
        print(f"[Startup] –û—à–∏–±–∫–∞ health check: {e}")

    # –ü–µ—Ä–µ—Å—Ç—Ä–æ–∏—Ç—å BM25 –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ (–∫–∞–∫ —Ç—ã –ø—Ä–æ—Å–∏–ª)
    print("\nüìä –ü–µ—Ä–µ—Å—Ç—Ä–æ–π–∫–∞ BM25 –∏–Ω–¥–µ–∫—Å–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ...")
    await rebuild_bm25()
    print("üìä BM25 –≥–æ—Ç–æ–≤.")

@app.post("/query", response_model=RAGResponse)
async def query_knowledge_base(req: QueryRequest):
    """
    Endpoint –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ø–æ–ª—è, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏—Å—ã–ª–∞–µ—Ç —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥:
    - query, top_k, category, use_hybrid, use_entities, similarity
    (—Ç–∞–∫–∂–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è —Å—Ç–∞—Ä–æ–µ –∏–º—è use_entity_extraction –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
    """
    if not req.query or not req.query.strip():
        raise HTTPException(status_code=400, detail="–ü—É—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å")

    # –ø–æ–¥–¥–µ—Ä–∂–∫–∞ –¥–≤—É—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ñ–ª–∞–≥–æ–≤ (legacy)
    use_entities_flag = req.use_entities if req.use_entities is not None else bool(req.use_entity_extraction)
    similarity = float(req.similarity) if req.similarity is not None else 0.3
    # ensure bounds
    if similarity < 0.0: similarity = 0.0
    if similarity > 1.0: similarity = 1.0

    # –≤—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –ø–æ–∏—Å–∫–∞
    documents, extracted_entities = retrieve_documents(
        req.query,
        top_k=req.top_k,
        category=req.category,
        use_hybrid=req.use_hybrid,
        use_entities=use_entities_flag,
        similarity=similarity
    )

    if not documents:
        return RAGResponse(
            answer="‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.",
            sources=[],
            query=req.query,
            extracted_entities=extracted_entities if extracted_entities else None
        )

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ (–≤–∫–ª—é—á–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –≤—Å—Ç–∞–≤–ª—è—Ç—å [1], [2] –≤ —Ç–µ–∫—Å—Ç)
    answer = generate_answer(req.query, documents, extracted_entities)

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è —Ñ—Ä–æ–Ω—Ç–µ–Ω–¥–∞ ‚Äî **–ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç** –Ω—É–∂–µ–Ω –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è STEP ID / UPDATED
    sources = [
        RetrievedDocument(
            text=doc["text"],
            score=doc.get("score", 0.0),
            category=doc.get("category", "unknown"),
            lesson=doc.get("lesson", "unknown"),
            chunk_id=doc.get("chunk_id", -1),
            search_method=doc.get("search_method", "unknown")
        )
        for doc in documents
    ]

    return RAGResponse(
        answer=answer,
        sources=sources,
        query=req.query,
        extracted_entities=extracted_entities if extracted_entities else None
    )

@app.get("/")
async def root():
    return {
        "service": "Hybrid RAG Knowledge Base API",
        "features": [
            "–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (Vector + BM25)",
            "–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ Gemini",
            "–ü–µ—Ä–µ—Å—Ç—Ä–æ–π–∫–∞ BM25 –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ",
            "–í—Å—Ç–∞–≤–∫–∞ —Ü–∏—Ñ—Ä–æ–≤—ã—Ö —Å—Å—ã–ª–æ–∫ [1],[2] –≤ —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞",
            "–ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (similarity) –ø–æ–¥–¥–µ—Ä–∂–∞–Ω –≤ –∑–∞–ø—Ä–æ—Å–µ"
        ],
        "endpoints": {
            "query": "POST /query - –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å",
            "health": "GET /health - –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ—Ä–≤–∏—Å–æ–≤",
            "rebuild_bm25": "POST /rebuild_bm25 - –ü–µ—Ä–µ—Å—Ç—Ä–æ–∏—Ç—å BM25 –∏–Ω–¥–µ–∫—Å"
        }
    }

if __name__ == "__main__":
    HOST = "0.0.0.0"
    PORT = int(os.getenv("PORT", 8001))
    print("=" * 70)
    print("üöÄ Hybrid RAG Knowledge Base Server")
    print("=" * 70)
    uvicorn.run(app, host=HOST, port=PORT)

